# GitHub Actions Workflow: Hourly Scraping
# Runs scraper every hour, saves to Neon DB
# FREE: 2000 minutes/month on GitHub

name: Hourly Bus Scraping

on:
  # Manual trigger
  workflow_dispatch:
  
  # Scheduled: Every hour
  schedule:
    - cron: '0 * * * *'

env:
  NEON_DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}
  SAVE_TO_DATABASE: 'true'

jobs:
  scrape:
    runs-on: ubuntu-22.04  # Use Ubuntu 22.04 for better compatibility
    timeout-minutes: 45
    
    steps:
      - name: ğŸ“¥ Checkout repository
        uses: actions/checkout@v4
      
      - name: ğŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: ğŸ“¦ Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install playwright pandas openpyxl python-dotenv
          pip install psycopg[binary] || pip install psycopg2-binary
      
      - name: ğŸ­ Install Playwright browsers
        run: |
          playwright install chromium
          playwright install-deps chromium
      
      - name: ğŸšŒ Run scraper
        run: |
          cd src
          xvfb-run --auto-servernum --server-args="-screen 0 1920x1080x24" python scraper.py --single-run
        env:
          NEON_DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}
          SAVE_TO_DATABASE: 'true'
          CI: 'true'
      
      - name: ğŸ“¤ Upload data artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: scraped-data-${{ github.run_number }}
          path: src/scraped_data/
          retention-days: 7
          if-no-files-found: ignore
