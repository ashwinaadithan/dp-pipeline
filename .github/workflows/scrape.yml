# GitHub Actions Workflow: Hourly Scraping
# Runs scraper every hour, saves to Neon DB
# FREE: 2000 minutes/month on GitHub

name: Hourly Bus Scraping

on:
  # Manual trigger
  workflow_dispatch:
  
  # Scheduled: Every hour
  schedule:
    - cron: '0 * * * *'  # At minute 0 of every hour

env:
  NEON_DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}
  SAVE_TO_DATABASE: 'true'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 45  # Prevent runaway jobs
    
    steps:
      # 1. Checkout code
      - name: ğŸ“¥ Checkout repository
        uses: actions/checkout@v4
      
      # 2. Setup Python
      - name: ğŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      # 3. Install system dependencies for Playwright
      - name: ğŸ”§ Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libasound2t64 libatk1.0-0 libcups2 libdrm2 libgbm1 libgtk-3-0 libnspr4 libnss3 libxcomposite1 libxdamage1 libxrandr2 xvfb
      
      # 4. Install Python dependencies
      - name: ğŸ“¦ Install dependencies
        run: |
          pip install --upgrade pip
          pip install playwright pandas openpyxl psycopg[binary] python-dotenv
          playwright install chromium
      
      # 5. Run scraper (single run mode with virtual display)
      - name: ğŸšŒ Run scraper
        run: |
          cd src
          xvfb-run --auto-servernum python scraper.py --single-run
        env:
          NEON_DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}
          SAVE_TO_DATABASE: 'true'
      
      # 6. Upload artifacts (backup)
      - name: ğŸ“¤ Upload data artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: scraped-data-${{ github.run_number }}
          path: src/scraped_data/
          retention-days: 7
      
      # 7. Commit data to repo (optional)
      - name: ğŸ’¾ Commit data to repository
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add src/scraped_data/ || true
          git diff --quiet && git diff --staged --quiet || git commit -m "ğŸšŒ Automated scrape: $(date +'%Y-%m-%d %H:%M')"
          git push || true
        continue-on-error: true
